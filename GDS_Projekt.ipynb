{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cleantext\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning med clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Sometimes the power of Christmas will make you...\n",
      "1    AWAKENING OF 12 STRANDS of DNA – “Reconnecting...\n",
      "2    Never Hike Alone: A Friday the 13th Fan Film U...\n",
      "3    When a rare shark was caught, scientists were ...\n",
      "4    Donald Trump has the unnerving ability to abil...\n",
      "Name: content, dtype: object\n",
      "0    sometim power christma make wild wonder thing ...\n",
      "1    awaken strand dna – “reconnect you” movi reade...\n",
      "2    never hike alon friday th fan film usa min fan...\n",
      "3    rare shark caught scientist left blunder answe...\n",
      "4    donald trump unnerv abil abil creat realiti co...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data_test.csv')\n",
    "print(df['content'].head(5))\n",
    "\n",
    "df['content'] = df['content'].apply(lambda x: cleantext.clean(text=x))\n",
    "print(df['content'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and structuring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning med clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [sometim, power, christma, make, wild, wonder,...\n",
      "1    [awaken, strand, dna, –, “reconnect, you”, mov...\n",
      "2    [never, hike, alon, friday, th, fan, film, usa...\n",
      "3    [rare, shark, caught, scientist, left, blunder...\n",
      "4    [donald, trump, unnerv, abil, abil, creat, rea...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['content'] = df['content'].apply(lambda x: cleantext.clean_words(\n",
    "    text=x,\n",
    "    clean_all=True,\n",
    "    extra_spaces=True,\n",
    "    stemming=True,\n",
    "    stopwords=True,\n",
    "    stp_lang='english',\n",
    "))\n",
    "\n",
    "print(df['content'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorization and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of fakes: 0.9796954314720813\n",
      "Ratio of real: 0.02030456852791878\n",
      "Number of articles left: 209\n"
     ]
    }
   ],
   "source": [
    "temp = []  # Temporary list to store filtered rows\n",
    "categories = set()  # Set to track unique categories\n",
    "convert = {  # Dictionary to map categories to standardized labels\n",
    "    '': \"\", 'conspiracy': 'fake', 'satire': 'fake', 'reliable': 'reliable',\n",
    "    'unreliable': 'skip', 'junksci': 'fake', 'unknown': 'skip',\n",
    "    'political': 'skip', 'fake': 'fake', 'hate': 'fake',\n",
    "    'clickbait': 'reliable', 'bias': 'skip', 'rumor': 'fake'\n",
    "}\n",
    "\n",
    "fakeCount = 0  # Counter for fake news articles\n",
    "realCount = 0  # Counter for reliable news articles\n",
    "\n",
    "with open(\"data_cleaned.csv\", \"r\") as src:  # Open CSV file for reading\n",
    "    reader = csv.reader(src)  # Create a CSV reader object\n",
    "    header = next(reader)  # Read and discard the header row\n",
    "    \n",
    "    for row in reader:  \n",
    "        content = row[4]  # Extract category from the fourth column\n",
    "        categories.add(content)  # Store the unique category\n",
    "        \n",
    "        row[4] = convert[row[4]]  # Convert category using the dictionary\n",
    "        \n",
    "        if row[4] == \"fake\":  \n",
    "            fakeCount += 1  # Increment fake count\n",
    "        elif row[4] == \"reliable\":\n",
    "            realCount += 1  # Increment real count\n",
    "        \n",
    "        if row[4] != \"skip\":  # Skip unwanted categories\n",
    "            temp.append(row)  # Append valid rows to the list\n",
    "\n",
    "# Create a DataFrame from the filtered list, keeping the original column names\n",
    "df = pd.DataFrame(temp, columns=header)\n",
    "\n",
    "# Save the processed data to a new CSV file without renaming columns\n",
    "df.to_csv('data_cleaned_fr.csv', index=False)\n",
    "\n",
    "# Print the ratio of fake vs. real news articles and number of articles\n",
    "print(f\"Ratio of fakes: {fakeCount/(fakeCount+realCount)}\")\n",
    "print(f\"Ratio of real: {realCount/(fakeCount+realCount)}\")\n",
    "print(f\"Number of articles left: {len(temp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Labels (fake or reliable)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create a CountVectorizer to convert text into a bag of words\u001b[39;00m\n\u001b[1;32m     12\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)  \u001b[38;5;66;03m# You can adjust max_features\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"output_file.csv\")\n",
    "\n",
    "# Prepare the data for the bag of words model\n",
    "X = train_data['content']  # Text data\n",
    "y = train_data['type']  # Labels (fake or reliable)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a CountVectorizer to convert text into a bag of words\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=5000)  # You can adjust max_features\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
