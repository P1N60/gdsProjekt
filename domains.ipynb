{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data_cleaned_fr_100000.csv\")\n",
    "train_data = df\n",
    "\n",
    "train_data['domain_names'] = train_data.domain.apply(lambda x: str(x))\n",
    "\n",
    "\n",
    "train_data['domain_names'] = train_data.domain.apply(lambda x: str(x))\n",
    "\n",
    "# Filter the data for reliable articles\n",
    "reliable_data = train_data[train_data['type'] == 'reliable']\n",
    "\n",
    "# Filter the data for fake articles\n",
    "fake_data = train_data[train_data['type'] == 'fake']\n",
    "\n",
    "# Group by domain and count the number of reliable articles per domain\n",
    "reliable_domain_counts = reliable_data.groupby('domain_names').size().reset_index(name='reliable_count')\n",
    "\n",
    "# Group by domain and count the number of fake articles per domain\n",
    "fake_domain_counts = fake_data.groupby('domain_names').size().reset_index(name='fake_count')\n",
    "\n",
    "# Merge the two DataFrames on domain names to combine reliable and fake counts\n",
    "combined_counts = pd.merge(reliable_domain_counts, fake_domain_counts, on='domain_names', how='outer')\n",
    "\n",
    "# Fill NaN values with 0 (in case some domains don't have either reliable or fake news)\n",
    "combined_counts = combined_counts.fillna(0)\n",
    "\n",
    "# Sort by the total count of reliable articles (or fake news, depending on your preference)\n",
    "top_10_real_combined = combined_counts.sort_values(by='reliable_count', ascending=False).head(10)\n",
    "\n",
    "# Sort by the total count of reliable articles (or fake news, depending on your preference)\n",
    "top_10_fake_combined = combined_counts.sort_values(by='fake_count', ascending=False).head(10)\n",
    "\n",
    "# Display the top 10 domains with their reliable and fake article counts\n",
    "print(top_10_real_combined)\n",
    "print(top_10_fake_combined)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.barh(top_10_real_combined['domain_names'], top_10_real_combined['reliable_count'], color='blue')\n",
    "\n",
    "# Labels and Title\n",
    "plt.xlabel(\"Reliable News Count\")\n",
    "plt.ylabel(\"Domain Names\")\n",
    "plt.title(\"Top 10 Domains with Most Reliable News Articles\")\n",
    "\n",
    "# Invert y-axis to have the largest at the top\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.barh(top_10_fake_combined['domain_names'], top_10_fake_combined['fake_count'], color='red')\n",
    "\n",
    "# Labels and Title\n",
    "plt.xlabel(\"Fake News Count\")\n",
    "plt.ylabel(\"Domain Names\")\n",
    "plt.title(\"Top 10 Domains with Most Fake News Articles\")\n",
    "\n",
    "# Invert y-axis to have the largest at the top\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
