{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data clearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cleantext\n",
    "import csv\n",
    "import sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a chunk size (e.g., 1000 rows per chunk, adjust based on file size)\n",
    "chunk_size = 1000\n",
    "rows = 995000\n",
    "chunks = []\n",
    "\n",
    "# Use tqdm to show progress while iterating over chunks\n",
    "with tqdm(desc=\"Loading csv file\", total=rows) as pbar:\n",
    "    for chunk in pd.read_csv(\"995,000_rows.csv\",nrows=rows, chunksize=chunk_size):\n",
    "        chunks.append(chunk)\n",
    "        pbar.update(chunk_size)\n",
    "\n",
    "# Combine all chunks into a single DataFrame\n",
    "df = pd.concat(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning med clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "df['content'] = df['content'].apply(lambda x: cleantext.clean(text=x) if isinstance(x, str) else None)\n",
    "df = df.dropna(subset=['content'])\n",
    "print(df['content'].head(5))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning med clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(lambda x: cleantext.clean_words(\n",
    "    text=x,\n",
    "    clean_all=True,\n",
    "    extra_spaces=True,\n",
    "    stemming=True,\n",
    "    stopwords=True,\n",
    "    stp_lang='english',\n",
    ") if isinstance(x, str) else None)\n",
    "df = df.dropna(subset=['content'])\n",
    "\n",
    "print(df['content'].head(5))\n",
    "print(\"\\nArticles Left: \", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on row:  ['908192', '', 'Financials   7:50am EST BRIEF-Al Tawfeek Co for Financial Leasing Q3 profit rises \\nNov 13 Al Tawfeek Company for Financial Leasing \\n* Q3 consol total operating revenue EGP 117.2 million versus EGP 102.2 million year ago \\n* Q3 consol net profit EGP 8.7 million versus EGP 8.4 million year ago Source: ( bit.ly/2etVQAu ) Further company coverage: Next In Financials', '2016-11-13T15:38:41.407+02:00', '2018-02-10 13:43:39.521661', '2018-02-10 13:43:39.521686', \"['briefal', 'tawfeek', 'co', 'financ', 'lea', 'q', 'profit', 'rise']\", '', '', '', '', '', '', 'webhose', '', '', '', '']\n",
      "Number of articles left: 868346\n",
      "Reliable articles: 573725\n",
      "Fake articles: 294621\n",
      "Ratio of fakes: 0.33928986832437763\n",
      "Ratio of reliable: 0.6607101316756224\n"
     ]
    }
   ],
   "source": [
    "temp = [] \n",
    "convert = {\n",
    "    '': \"skip\", 'conspiracy': 'fake', 'satire': 'fake', 'reliable': 'reliable',\n",
    "    'unreliable': 'skip', 'junksci': 'fake', 'unknown': 'skip',\n",
    "    'political': 'reliable', 'fake': 'fake', 'hate': 'fake',\n",
    "    'clickbait': 'reliable', 'bias': 'reliable', 'rumor': 'fake'\n",
    "}\n",
    "\n",
    "fakeCount = 0 \n",
    "realCount = 0 \n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "with open(\"data_cleaned.csv\", \"r\") as src: \n",
    "    reader = csv.reader(src) \n",
    "    header = next(reader)\n",
    "        \n",
    "    for row in reader:  \n",
    "        try:\n",
    "            row[4] = convert[row[4]]  # Convert category using the dictionary\n",
    "                \n",
    "            if row[4] == \"fake\":   # 0 for fake\n",
    "                fakeCount += 1  # Increment fake count\n",
    "            elif row[4] == \"reliable\": # 1 for reliable\n",
    "                realCount += 1  # Increment real count\n",
    "                \n",
    "            if row[4] != \"skip\":  # Skip unwanted categories\n",
    "                temp.append(row)  # Append valid rows to the list\n",
    "                # print(row[4])\n",
    "        except Exception as e:\n",
    "            # Skip the bad rows\n",
    "            print(\"Error on row: \", row)\n",
    "            continue\n",
    "            \n",
    "\n",
    "# Create a DataFrame from the filtered list, keeping the original column names\n",
    "df = pd.DataFrame(temp, columns=header)\n",
    "\n",
    "# Print the ratio of fake vs. real news articles and number of articles\n",
    "print(f\"Number of articles left: {len(temp)}\")\n",
    "print(f\"Reliable articles: {realCount}\")\n",
    "print(f\"Fake articles: {fakeCount}\")\n",
    "print(f\"Ratio of fakes: {fakeCount/(fakeCount+realCount)}\")\n",
    "print(f\"Ratio of reliable: {realCount/(fakeCount+realCount)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(294621, 573725)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fakeCount, realCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data_cleaned_fr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data_cleaned_fr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
